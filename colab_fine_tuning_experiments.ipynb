{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## RUN THIS BEFORE ANYTHING\n",
        "# Create a virtual environment named 'synth_env'\n",
        "!apt install python3.10-venv\n",
        "!python -m venv synth_env\n",
        "\n",
        "# Activate the virtual environment and install necessary libraries\n",
        "!source synth_env/bin/activate && pip install transformers datasets audioread accelerate peft torchaudio scikit-learn evaluate jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cudpCXr9V6tA",
        "outputId": "fbbb87bc-05c4-4d30-deb9-013496c4e5c3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "python3.10-venv is already the newest version (3.10.12-1~22.04.7).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
            "Requirement already satisfied: datasets in ./synth_env/lib/python3.10/site-packages (3.1.0)\n",
            "Requirement already satisfied: audioread in ./synth_env/lib/python3.10/site-packages (3.0.1)\n",
            "Requirement already satisfied: accelerate in ./synth_env/lib/python3.10/site-packages (1.1.1)\n",
            "Requirement already satisfied: peft in ./synth_env/lib/python3.10/site-packages (0.13.2)\n",
            "Requirement already satisfied: torchaudio in ./synth_env/lib/python3.10/site-packages (2.5.1)\n",
            "Requirement already satisfied: scikit-learn in ./synth_env/lib/python3.10/site-packages (1.5.2)\n",
            "Requirement already satisfied: evaluate in ./synth_env/lib/python3.10/site-packages (0.4.3)\n",
            "Requirement already satisfied: jiwer in ./synth_env/lib/python3.10/site-packages (3.0.5)\n",
            "Requirement already satisfied: requests in ./synth_env/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.17 in ./synth_env/lib/python3.10/site-packages (from transformers) (2.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in ./synth_env/lib/python3.10/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in ./synth_env/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: filelock in ./synth_env/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./synth_env/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./synth_env/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./synth_env/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
            "Collecting tokenizers<0.21,>=0.20\n",
            "  Using cached tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./synth_env/lib/python3.10/site-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./synth_env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in ./synth_env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: pandas in ./synth_env/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in ./synth_env/lib/python3.10/site-packages (from datasets) (2024.9.0)\n",
            "Requirement already satisfied: xxhash in ./synth_env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in ./synth_env/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in ./synth_env/lib/python3.10/site-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: psutil in ./synth_env/lib/python3.10/site-packages (from accelerate) (6.1.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in ./synth_env/lib/python3.10/site-packages (from accelerate) (2.5.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: triton==3.1.0 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.0)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: networkx in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: jinja2 in ./synth_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./synth_env/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./synth_env/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in ./synth_env/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in ./synth_env/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in ./synth_env/lib/python3.10/site-packages (from jiwer) (8.1.7)\n",
            "Requirement already satisfied: rapidfuzz<4,>=3 in ./synth_env/lib/python3.10/site-packages (from jiwer) (3.10.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./synth_env/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./synth_env/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./synth_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./synth_env/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./synth_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./synth_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./synth_env/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./synth_env/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./synth_env/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./synth_env/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./synth_env/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./synth_env/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./synth_env/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./synth_env/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./synth_env/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in ./synth_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./synth_env/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "Successfully installed tokenizers-0.20.3 transformers-4.46.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Check necessary libraries\n",
        "!source synth_env/bin/activate && pip list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__92_nKeHYYh",
        "outputId": "166c1e7a-25b8-4669-f8be-72b1e9ff2e14"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                  Version\n",
            "------------------------ -----------\n",
            "accelerate               1.1.1\n",
            "aiohappyeyeballs         2.4.4\n",
            "aiohttp                  3.11.9\n",
            "aiosignal                1.3.1\n",
            "async-timeout            5.0.1\n",
            "attrs                    24.2.0\n",
            "audioread                3.0.1\n",
            "certifi                  2024.8.30\n",
            "charset-normalizer       3.4.0\n",
            "click                    8.1.7\n",
            "datasets                 3.1.0\n",
            "dill                     0.3.8\n",
            "evaluate                 0.4.3\n",
            "filelock                 3.16.1\n",
            "frozenlist               1.5.0\n",
            "fsspec                   2024.9.0\n",
            "huggingface-hub          0.26.3\n",
            "idna                     3.10\n",
            "Jinja2                   3.1.4\n",
            "jiwer                    3.0.5\n",
            "joblib                   1.4.2\n",
            "MarkupSafe               3.0.2\n",
            "mpmath                   1.3.0\n",
            "multidict                6.1.0\n",
            "multiprocess             0.70.16\n",
            "networkx                 3.4.2\n",
            "numpy                    2.1.3\n",
            "nvidia-cublas-cu12       12.4.5.8\n",
            "nvidia-cuda-cupti-cu12   12.4.127\n",
            "nvidia-cuda-nvrtc-cu12   12.4.127\n",
            "nvidia-cuda-runtime-cu12 12.4.127\n",
            "nvidia-cudnn-cu12        9.1.0.70\n",
            "nvidia-cufft-cu12        11.2.1.3\n",
            "nvidia-curand-cu12       10.3.5.147\n",
            "nvidia-cusolver-cu12     11.6.1.9\n",
            "nvidia-cusparse-cu12     12.3.1.170\n",
            "nvidia-nccl-cu12         2.21.5\n",
            "nvidia-nvjitlink-cu12    12.4.127\n",
            "nvidia-nvtx-cu12         12.4.127\n",
            "packaging                24.2\n",
            "pandas                   2.2.3\n",
            "peft                     0.13.2\n",
            "pip                      22.0.2\n",
            "propcache                0.2.1\n",
            "psutil                   6.1.0\n",
            "pyarrow                  18.1.0\n",
            "python-dateutil          2.9.0.post0\n",
            "pytz                     2024.2\n",
            "PyYAML                   6.0.2\n",
            "RapidFuzz                3.10.1\n",
            "regex                    2024.11.6\n",
            "requests                 2.32.3\n",
            "safetensors              0.4.5\n",
            "scikit-learn             1.5.2\n",
            "scipy                    1.14.1\n",
            "setuptools               59.6.0\n",
            "six                      1.17.0\n",
            "sympy                    1.13.1\n",
            "threadpoolctl            3.5.0\n",
            "tokenizers               0.13.3\n",
            "torch                    2.5.1\n",
            "torchaudio               2.5.1\n",
            "tqdm                     4.67.1\n",
            "transformers             4.33.0\n",
            "triton                   3.1.0\n",
            "typing_extensions        4.12.2\n",
            "tzdata                   2024.2\n",
            "urllib3                  2.2.3\n",
            "xxhash                   3.5.0\n",
            "yarl                     1.18.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (DO THIS BEFORE NEXT STEP)\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Cy6aRCiLIeE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72dc4eb7-d289-4c4b-be6a-4db159474f9b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create CSV file for FINE TUNING\n",
        "\n",
        "import os\n",
        "import csv\n",
        "\n",
        "# Define the directory containing the dataset in Google Drive\n",
        "dataset_dir = \"/content/drive/MyDrive/whisper_synth_files/data/dataset1\"\n",
        "\n",
        "# Create a list to store the data rows\n",
        "data_rows = []\n",
        "\n",
        "# Loop through all files in the dataset directory\n",
        "for filename in os.listdir(dataset_dir):\n",
        "    if filename.endswith(\".wav\"):\n",
        "        # Extract the base name (e.g., 'sound1')\n",
        "        base_name = os.path.splitext(filename)[0]\n",
        "        text_filename = base_name + \".txt\"\n",
        "        text_filepath = os.path.join(dataset_dir, text_filename)\n",
        "        if os.path.exists(text_filepath):\n",
        "            audio_filepath = os.path.join(dataset_dir, filename)\n",
        "            with open(text_filepath, \"r\") as text_file:\n",
        "                text_content = text_file.read().strip()\n",
        "\n",
        "            # Add the audio path and text content to the data rows\n",
        "            data_rows.append([audio_filepath, text_content])\n",
        "\n",
        "# Define the CSV file path\n",
        "csv_file_path = os.path.join(dataset_dir, \"train.csv\")\n",
        "\n",
        "# Write the data rows to the CSV file\n",
        "with open(csv_file_path, \"w\", newline=\"\") as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow([\"audio\", \"text\"])\n",
        "    csv_writer.writerows(data_rows)\n",
        "\n",
        "print(f\"CSV file 'train.csv' has been created at: {csv_file_path}\")"
      ],
      "metadata": {
        "id": "WrOjfomvNWVK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "380f4356-7f69-4313-da9b-f853c1354f48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file 'train.csv' has been created at: /content/drive/MyDrive/whisper_synth_files/data/dataset1/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate the virtual environment and view model architechture\n",
        "!source synth_env/bin/activate && python -c \"from transformers import WhisperForConditionalGeneration; model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small'); print(model)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6yS3EdqdHGu",
        "outputId": "b6b03ed0-f8bc-44f2-cad5-4b1835915750"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/synth_env/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/content/synth_env/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/content/synth_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "WhisperForConditionalGeneration(\n",
            "  (model): WhisperModel(\n",
            "    (encoder): WhisperEncoder(\n",
            "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "      (embed_positions): Embedding(1500, 768)\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x WhisperEncoderLayer(\n",
            "          (self_attn): WhisperAttention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (activation_fn): GELUActivation()\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (decoder): WhisperDecoder(\n",
            "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
            "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x WhisperDecoderLayer(\n",
            "          (self_attn): WhisperAttention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (activation_fn): GELUActivation()\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): WhisperAttention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SCRIPT FOR ADDING SPECIAL TOKENS add_tokens.py\n",
        "\n",
        "with open(\"add_tokens.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "from transformers import WhisperTokenizer, WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "# Load the base tokenizer and model\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "\n",
        "# Define special tokens\n",
        "special_tokens = [\n",
        "    # Tokens without leading spaces\n",
        "    \"Waveform:\", \"Voices:\", \"Oscillator Detune:\", \"Filter Type:\", \"Filter Cutoff:\",\n",
        "    \"ADSR Envelope:\", \"Attack:\", \"Decay:\", \"Sustain:\", \"Release:\",\n",
        "    \"LFO Modulation:\", \"Hz\", \"ms\", \"dB\", \"s\", \",\", \"\\\\n\", \"-\", \".\", \"None\",\n",
        "    # Tokens with leading spaces\n",
        "    \" Waveform:\", \" Voices:\", \" Oscillator Detune:\", \" Filter Type:\", \" Filter Cutoff:\",\n",
        "    \" ADSR Envelope:\", \" Attack:\", \" Decay:\", \" Sustain:\", \" Release:\",\n",
        "    \" LFO Modulation:\", \" Hz\", \" ms\", \" dB\", \" s\", \" None\", \" -\"\n",
        "]\n",
        "\n",
        "# Add special tokens to the tokenizer\n",
        "num_added_toks = tokenizer.add_tokens(special_tokens)\n",
        "print(f\"Added {num_added_toks} tokens\")\n",
        "\n",
        "# Resize the model's embeddings to accommodate new tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Create a processor with the updated tokenizer\n",
        "processor = WhisperProcessor.from_pretrained(\n",
        "    \"openai/whisper-small\",\n",
        "    language=\"en\",\n",
        "    task=\"transcribe\",\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Save the tokenizer and processor\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/whisper_synth_files/whisper_tokenizer_with_special_tokens\")\n",
        "processor.save_pretrained(\"/content/drive/MyDrive/whisper_synth_files/whisper_processor_with_special_tokens\")\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "IaHN5S55gc3D"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate the environment and run the script\n",
        "!source synth_env/bin/activate && python add_tokens.py"
      ],
      "metadata": {
        "id": "cvEP1dFSgq0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ddb8281-cdf8-4261-c9e9-4ac92815d2be"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 31 tokens\n",
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer test\n",
        "with open(\"tokenizer_test.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "from transformers import WhisperTokenizer\n",
        "\n",
        "# Load the tokenizer with added special tokens\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"/content/drive/MyDrive/whisper_synth_files/whisper_tokenizer_with_special_tokens\")\n",
        "\n",
        "# Test text\n",
        "test_text = \"Sustain: -12.3 dB\"\n",
        "\n",
        "# Tokenize the test text\n",
        "tokens = tokenizer.tokenize(test_text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Optionally, print token IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Token IDs:\", token_ids)\n",
        "\"\"\")\n",
        "\n",
        "!source synth_env/bin/activate && python tokenizer_test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSfXkmDrvY4B",
        "outputId": "1693f402-a208-4fd0-e0f1-8aa5e8360e30"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Sustain:', '-', '12', '.', '3', ' dB']\n",
            "Token IDs: [51873, 12, 4762, 13, 18, 51892]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SCRIPT FOR FINE TUNING (USING LORA) finetune_whisper.py\n",
        "\n",
        "# Create a script to perform the entire fine-tuning\n",
        "with open(\"finetune_whisper.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    WhisperForConditionalGeneration,\n",
        "    WhisperProcessor,\n",
        "    WhisperTokenizer,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig\n",
        "import torchaudio\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "# Define paths\n",
        "data_path = '/content/drive/MyDrive/whisper_synth_files/data/dataset1/'\n",
        "\n",
        "# Load dataset\n",
        "data_files = {'train': data_path + 'train.csv'}\n",
        "dataset = load_dataset('csv', data_files=data_files)\n",
        "\n",
        "# Split the dataset into training and evaluation sets\n",
        "train_data, eval_data = train_test_split(dataset['train'].to_pandas(), test_size=0.2)\n",
        "\n",
        "# Convert train and eval data back to Dataset objects\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame(train_data))\n",
        "eval_dataset = Dataset.from_pandas(pd.DataFrame(eval_data))\n",
        "\n",
        "# Load the updated tokenizer\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"/content/drive/MyDrive/whisper_synth_files/whisper_tokenizer_with_special_tokens\")\n",
        "\n",
        "# Load the processor with the updated tokenizer\n",
        "processor = WhisperProcessor.from_pretrained(\n",
        "    \"openai/whisper-small\",\n",
        "    language=\"en\",\n",
        "    task=\"transcribe\",\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Load the model\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "\n",
        "# Resize the model's embeddings to accommodate new tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    audio_path = examples['audio']\n",
        "    audio_array, sampling_rate = torchaudio.load(audio_path)\n",
        "\n",
        "    if sampling_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
        "        audio_array = resampler(audio_array)\n",
        "\n",
        "    audio_array = audio_array.squeeze().numpy()\n",
        "\n",
        "    # Extract input features\n",
        "    input_features = processor.feature_extractor(\n",
        "        audio_array, sampling_rate=16000\n",
        "    ).input_features[0]\n",
        "\n",
        "    # Tokenize target text\n",
        "    text = examples['text']\n",
        "    labels = processor.tokenizer(\n",
        "        text\n",
        "    ).input_ids\n",
        "\n",
        "    # Return a dictionary with the correct keys\n",
        "    return {\n",
        "        \"input_features\": input_features,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "# Preprocess datasets\n",
        "train_dataset = train_dataset.map(preprocess_function, remove_columns=['audio', 'text'])\n",
        "eval_dataset = eval_dataset.map(preprocess_function, remove_columns=['audio', 'text'])\n",
        "\n",
        "# Apply LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\"\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Define custom data collator\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "    decoder_start_token_id: int\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # Separate input_features and labels\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        labels = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        # Pad input_features using the feature extractor\n",
        "        batch = self.processor.feature_extractor.pad(\n",
        "            input_features,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Pad labels using the tokenizer\n",
        "        labels_batch = self.processor.tokenizer.pad(\n",
        "            labels,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Replace padding token id's of the labels by -100\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
        "            labels_batch.attention_mask.ne(1), -100\n",
        "        )\n",
        "\n",
        "        # Remove the decoder_start_token_id\n",
        "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "# Initialize the data collator\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
        "    processor=processor,\n",
        "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/whisper_synth_files/whisper_finetuned\",\n",
        "    per_device_train_batch_size=4,\n",
        "    learning_rate=3e-5,\n",
        "    num_train_epochs=10,\n",
        "    logging_dir=\"/content/drive/MyDrive/whisper_synth_files/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    save_steps=500,\n",
        "    eval_strategy=\"epoch\",\n",
        "    eval_steps=500,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "# Define custom metric\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    # Replace -100 with pad_token_id\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_str = processor.tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Calculate exact match accuracy\n",
        "    exact_matches = [int(p.strip() == l.strip()) for p, l in zip(pred_str, labels_str)]\n",
        "    accuracy = sum(exact_matches) / len(exact_matches)\n",
        "\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    processing_class=processor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "CSRIROgSG71a"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate the environment and run the script\n",
        "!source synth_env/bin/activate && python finetune_whisper.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjmkc-t9Y2n-",
        "outputId": "44887848-477c-4280-e1c8-237da138b2f2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "Map: 100% 160/160 [00:04<00:00, 37.49 examples/s]\n",
            "Map: 100% 40/40 [00:01<00:00, 37.50 examples/s]\n",
            "  0% 0/400 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "{'loss': 10.2973, 'grad_norm': 11.144461631774902, 'learning_rate': 2.9325e-05, 'epoch': 0.25}\n",
            "{'loss': 9.275, 'grad_norm': 7.985929012298584, 'learning_rate': 2.8575e-05, 'epoch': 0.5}\n",
            "{'loss': 8.6888, 'grad_norm': 9.840339660644531, 'learning_rate': 2.805e-05, 'epoch': 0.75}\n",
            "{'loss': 8.2177, 'grad_norm': 4.99782133102417, 'learning_rate': 2.7300000000000003e-05, 'epoch': 1.0}\n",
            " 10% 40/400 [00:29<03:24,  1.76it/s]Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 40% 2/5 [00:01<00:02,  1.31it/s]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 60% 3/5 [00:03<00:02,  1.08s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 80% 4/5 [00:04<00:01,  1.25s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 7.881361484527588, 'eval_accuracy': 0.0, 'eval_runtime': 11.158, 'eval_samples_per_second': 3.585, 'eval_steps_per_second': 0.448, 'epoch': 1.0}\n",
            " 10% 40/400 [00:40<03:24,  1.76it/s]\n",
            "100% 5/5 [00:08<00:00,  1.00s/it]\u001b[A\n",
            "{'loss': 7.9385, 'grad_norm': 5.356051921844482, 'learning_rate': 2.655e-05, 'epoch': 1.25}\n",
            "{'loss': 7.5249, 'grad_norm': 4.878258228302002, 'learning_rate': 2.58e-05, 'epoch': 1.5}\n",
            "{'loss': 7.4684, 'grad_norm': 4.9382004737854, 'learning_rate': 2.505e-05, 'epoch': 1.75}\n",
            "{'loss': 7.1981, 'grad_norm': 4.603023052215576, 'learning_rate': 2.43e-05, 'epoch': 2.0}\n",
            " 20% 80/400 [01:09<03:08,  1.70it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 40% 2/5 [00:13<00:19,  6.58s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 60% 3/5 [00:26<00:18,  9.31s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 80% 4/5 [00:39<00:10, 10.75s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 6.963015556335449, 'eval_accuracy': 0.0, 'eval_runtime': 58.4932, 'eval_samples_per_second': 0.684, 'eval_steps_per_second': 0.085, 'epoch': 2.0}\n",
            " 20% 80/400 [02:07<03:08,  1.70it/s]\n",
            "100% 5/5 [00:55<00:00, 11.25s/it]\u001b[A\n",
            "{'loss': 7.0305, 'grad_norm': 5.198399543762207, 'learning_rate': 2.3550000000000003e-05, 'epoch': 2.25}\n",
            "{'loss': 6.8372, 'grad_norm': 5.282978534698486, 'learning_rate': 2.2800000000000002e-05, 'epoch': 2.5}\n",
            "{'loss': 6.6826, 'grad_norm': 8.11627197265625, 'learning_rate': 2.205e-05, 'epoch': 2.75}\n",
            "{'loss': 6.3609, 'grad_norm': 8.088552474975586, 'learning_rate': 2.13e-05, 'epoch': 3.0}\n",
            " 30% 120/400 [02:36<02:36,  1.79it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 40% 2/5 [00:13<00:19,  6.61s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 60% 3/5 [00:26<00:18,  9.42s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 80% 4/5 [00:39<00:10, 10.84s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 6.2853899002075195, 'eval_accuracy': 0.0, 'eval_runtime': 74.6506, 'eval_samples_per_second': 0.536, 'eval_steps_per_second': 0.067, 'epoch': 3.0}\n",
            " 30% 120/400 [03:50<02:36,  1.79it/s]\n",
            "100% 5/5 [01:00<00:00, 11.31s/it]\u001b[A\n",
            "{'loss': 6.296, 'grad_norm': 5.790585517883301, 'learning_rate': 2.055e-05, 'epoch': 3.25}\n",
            "{'loss': 6.2891, 'grad_norm': 11.099038124084473, 'learning_rate': 1.98e-05, 'epoch': 3.5}\n",
            "{'loss': 6.209, 'grad_norm': 11.757949829101562, 'learning_rate': 1.9050000000000002e-05, 'epoch': 3.75}\n",
            "{'loss': 6.1502, 'grad_norm': 19.790451049804688, 'learning_rate': 1.83e-05, 'epoch': 4.0}\n",
            " 40% 160/400 [04:18<02:12,  1.81it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 40% 2/5 [00:13<00:19,  6.60s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 60% 3/5 [00:26<00:18,  9.37s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 80% 4/5 [00:39<00:10, 10.89s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 6.017542839050293, 'eval_accuracy': 0.0, 'eval_runtime': 77.5631, 'eval_samples_per_second': 0.516, 'eval_steps_per_second': 0.064, 'epoch': 4.0}\n",
            " 40% 160/400 [05:36<02:12,  1.81it/s]\n",
            "100% 5/5 [01:03<00:00, 11.39s/it]\u001b[A\n",
            "{'loss': 6.0496, 'grad_norm': 15.936274528503418, 'learning_rate': 1.755e-05, 'epoch': 4.25}\n",
            "{'loss': 6.092, 'grad_norm': 7.514286994934082, 'learning_rate': 1.6800000000000002e-05, 'epoch': 4.5}\n",
            "{'loss': 6.0616, 'grad_norm': 15.577880859375, 'learning_rate': 1.605e-05, 'epoch': 4.75}\n",
            "{'loss': 5.9329, 'grad_norm': 10.71818733215332, 'learning_rate': 1.53e-05, 'epoch': 5.0}\n",
            " 50% 200/400 [06:04<01:53,  1.77it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 40% 2/5 [00:13<00:19,  6.65s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 60% 3/5 [00:26<00:18,  9.40s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 80% 4/5 [00:39<00:10, 10.82s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 5.878827095031738, 'eval_accuracy': 0.0, 'eval_runtime': 75.6478, 'eval_samples_per_second': 0.529, 'eval_steps_per_second': 0.066, 'epoch': 5.0}\n",
            " 50% 200/400 [07:20<01:53,  1.77it/s]\n",
            "100% 5/5 [01:01<00:00, 11.36s/it]\u001b[A\n",
            "{'loss': 5.9926, 'grad_norm': 14.472430229187012, 'learning_rate': 1.455e-05, 'epoch': 5.25}\n",
            "{'loss': 6.0027, 'grad_norm': 13.222064018249512, 'learning_rate': 1.3800000000000002e-05, 'epoch': 5.5}\n",
            "{'loss': 5.855, 'grad_norm': 8.971526145935059, 'learning_rate': 1.305e-05, 'epoch': 5.75}\n",
            "{'loss': 5.8312, 'grad_norm': 7.1103363037109375, 'learning_rate': 1.2299999999999999e-05, 'epoch': 6.0}\n",
            " 60% 240/400 [07:47<01:28,  1.81it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 40% 2/5 [00:13<00:19,  6.60s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 60% 3/5 [00:26<00:18,  9.36s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 80% 4/5 [00:39<00:10, 10.80s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 5.796543598175049, 'eval_accuracy': 0.0, 'eval_runtime': 74.9183, 'eval_samples_per_second': 0.534, 'eval_steps_per_second': 0.067, 'epoch': 6.0}\n",
            " 60% 240/400 [09:02<01:28,  1.81it/s]\n",
            "100% 5/5 [01:00<00:00, 11.28s/it]\u001b[A\n",
            "{'loss': 5.8531, 'grad_norm': 7.58134651184082, 'learning_rate': 1.1550000000000001e-05, 'epoch': 6.25}\n",
            "{'loss': 5.8525, 'grad_norm': 7.627947807312012, 'learning_rate': 1.08e-05, 'epoch': 6.5}\n",
            "{'loss': 5.8466, 'grad_norm': 9.178064346313477, 'learning_rate': 1.005e-05, 'epoch': 6.75}\n",
            "{'loss': 5.8164, 'grad_norm': 7.693061351776123, 'learning_rate': 9.3e-06, 'epoch': 7.0}\n",
            " 70% 280/400 [09:30<01:06,  1.82it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 40% 2/5 [00:13<00:19,  6.60s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 60% 3/5 [00:26<00:18,  9.32s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 80% 4/5 [00:39<00:10, 10.79s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 5.747781753540039, 'eval_accuracy': 0.0, 'eval_runtime': 76.0713, 'eval_samples_per_second': 0.526, 'eval_steps_per_second': 0.066, 'epoch': 7.0}\n",
            " 70% 280/400 [10:46<01:06,  1.82it/s]\n",
            "100% 5/5 [01:01<00:00, 11.28s/it]\u001b[A\n",
            "{'loss': 5.7546, 'grad_norm': 10.808076858520508, 'learning_rate': 8.55e-06, 'epoch': 7.25}\n",
            "{'loss': 5.8458, 'grad_norm': 10.671449661254883, 'learning_rate': 7.8e-06, 'epoch': 7.5}\n",
            "{'loss': 5.7898, 'grad_norm': 8.040266990661621, 'learning_rate': 7.049999999999999e-06, 'epoch': 7.75}\n",
            "{'loss': 5.7788, 'grad_norm': 7.871667861938477, 'learning_rate': 6.3e-06, 'epoch': 8.0}\n",
            " 80% 320/400 [11:14<00:44,  1.81it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 40% 2/5 [00:13<00:19,  6.55s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 60% 3/5 [00:26<00:18,  9.29s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 80% 4/5 [00:39<00:10, 10.78s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 5.708807468414307, 'eval_accuracy': 0.0, 'eval_runtime': 76.0629, 'eval_samples_per_second': 0.526, 'eval_steps_per_second': 0.066, 'epoch': 8.0}\n",
            " 80% 320/400 [12:30<00:44,  1.81it/s]\n",
            "100% 5/5 [01:01<00:00, 11.28s/it]\u001b[A\n",
            "{'loss': 5.753, 'grad_norm': 8.798097610473633, 'learning_rate': 5.55e-06, 'epoch': 8.25}\n",
            "{'loss': 5.7186, 'grad_norm': 10.086979866027832, 'learning_rate': 4.800000000000001e-06, 'epoch': 8.5}\n",
            "{'loss': 5.8041, 'grad_norm': 9.01584243774414, 'learning_rate': 4.05e-06, 'epoch': 8.75}\n",
            "{'loss': 5.7663, 'grad_norm': 7.929320335388184, 'learning_rate': 3.3e-06, 'epoch': 9.0}\n",
            " 90% 360/400 [12:58<00:22,  1.78it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 40% 2/5 [00:13<00:19,  6.58s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 60% 3/5 [00:26<00:18,  9.31s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 80% 4/5 [00:39<00:10, 10.75s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 5.692476749420166, 'eval_accuracy': 0.0, 'eval_runtime': 76.5617, 'eval_samples_per_second': 0.522, 'eval_steps_per_second': 0.065, 'epoch': 9.0}\n",
            " 90% 360/400 [14:14<00:22,  1.78it/s]\n",
            "100% 5/5 [01:02<00:00, 11.25s/it]\u001b[A\n",
            "{'loss': 5.7493, 'grad_norm': 7.0522236824035645, 'learning_rate': 2.55e-06, 'epoch': 9.25}\n",
            "{'loss': 5.6538, 'grad_norm': 8.350886344909668, 'learning_rate': 1.8e-06, 'epoch': 9.5}\n",
            "{'loss': 5.7924, 'grad_norm': 8.320603370666504, 'learning_rate': 1.0500000000000001e-06, 'epoch': 9.75}\n",
            "{'loss': 5.7762, 'grad_norm': 8.396358489990234, 'learning_rate': 3.0000000000000004e-07, 'epoch': 10.0}\n",
            "100% 400/400 [14:42<00:00,  1.80it/s]/content/synth_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 40% 2/5 [00:13<00:19,  6.61s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 60% 3/5 [00:26<00:18,  9.37s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            " 80% 4/5 [00:39<00:10, 10.84s/it]\u001b[ATrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 5.684231758117676, 'eval_accuracy': 0.0, 'eval_runtime': 76.8972, 'eval_samples_per_second': 0.52, 'eval_steps_per_second': 0.065, 'epoch': 10.0}\n",
            "100% 400/400 [16:01<00:00,  1.80it/s]\n",
            "100% 5/5 [01:02<00:00, 11.34s/it]\u001b[A\n",
            "{'train_runtime': 961.7096, 'train_samples_per_second': 1.664, 'train_steps_per_second': 0.416, 'train_loss': 6.470827112197876, 'epoch': 10.0}\n",
            "100% 400/400 [16:01<00:00,  2.40s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Create a script to combine with base model\n",
        "with open(\"combine_models.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import os\n",
        "import torch\n",
        "from transformers import WhisperForConditionalGeneration, WhisperTokenizer, WhisperProcessor\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load the tokenizer used during fine-tuning\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"/content/drive/MyDrive/whisper_synth_files/whisper_tokenizer_with_special_tokens\")\n",
        "\n",
        "# Load the base model and resize embeddings\n",
        "base_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Load the PeftModel with the adapters\n",
        "adapter_checkpoint = \"/content/drive/MyDrive/whisper_synth_files/whisper_finetuned/checkpoint-400\"\n",
        "\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    adapter_checkpoint\n",
        ")\n",
        "\n",
        "# Merge adapter weights with the base model\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save the complete model for future use\n",
        "save_path = \"/content/drive/MyDrive/whisper_synth_files/whisper_synth\"\n",
        "model.save_pretrained(save_path)\n",
        "\n",
        "# Also save the processor with the correct tokenizer\n",
        "processor = WhisperProcessor.from_pretrained(\n",
        "    \"openai/whisper-small\",\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "processor.save_pretrained(save_path)\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "2fYQeJpDE6vZ"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source synth_env/bin/activate && python combine_models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKV-qTkIFKbu",
        "outputId": "5336c2f1-f594-4f85-d614-aa97e49c1412"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "/content/synth_env/lib/python3.10/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SCRIPT FOR EVALUATION (eval.py)\n",
        "\n",
        "# Create a script to perform evaluation\n",
        "with open(\"eval.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    WhisperForConditionalGeneration,\n",
        "    WhisperProcessor,\n",
        "    WhisperTokenizer,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments\n",
        ")\n",
        "import evaluate\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "# Define paths\n",
        "data_path = '/content/drive/MyDrive/whisper_synth_files/data/dataset1/'\n",
        "\n",
        "# Load the dataset\n",
        "data_files = {'train': data_path + 'train.csv'}\n",
        "dataset = load_dataset('csv', data_files=data_files)\n",
        "\n",
        "# Split the dataset into training and evaluation sets\n",
        "_, eval_data = train_test_split(dataset['train'].to_pandas(), test_size=0.2)\n",
        "\n",
        "# Convert eval data back to Dataset object\n",
        "eval_dataset = Dataset.from_pandas(pd.DataFrame(eval_data))\n",
        "\n",
        "# Load the tokenizer and processor from the combined model directory\n",
        "save_path = \"/content/drive/MyDrive/whisper_synth_files/whisper_synth\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = WhisperTokenizer.from_pretrained(save_path)\n",
        "\n",
        "# Load processor and model for evaluation\n",
        "processor = WhisperProcessor.from_pretrained(\n",
        "    save_path,\n",
        "    language=\"en\",\n",
        "    task=\"transcribe\",\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Load the combined model\n",
        "model = WhisperForConditionalGeneration.from_pretrained(save_path)\n",
        "\n",
        "# Force the model to generate in English\n",
        "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
        "    language=\"en\", task=\"transcribe\"\n",
        ")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    audio_path = examples['audio']\n",
        "    audio_array, sampling_rate = torchaudio.load(audio_path)\n",
        "\n",
        "    # Resample if needed\n",
        "    if sampling_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
        "        audio_array = resampler(audio_array)\n",
        "\n",
        "    audio_array = audio_array.squeeze().numpy()\n",
        "\n",
        "    # Extract input features (log-Mel spectrogram)\n",
        "    input_features = processor.feature_extractor(\n",
        "        audio_array, sampling_rate=16000\n",
        "    ).input_features[0]\n",
        "\n",
        "    # Tokenize target text to create decoder input IDs\n",
        "    text = examples['text']\n",
        "    labels = processor.tokenizer(\n",
        "        text\n",
        "    ).input_ids\n",
        "\n",
        "    # Return a dictionary with the correct keys\n",
        "    return {\n",
        "        \"input_features\": input_features,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "# Preprocess eval dataset\n",
        "eval_dataset = eval_dataset.map(preprocess_function, remove_columns=['audio', 'text'])\n",
        "\n",
        "# Helper function to parse parameters from structured text\n",
        "def parse_parameters(text):\n",
        "    params = {}\n",
        "    lines = text.strip().split('\\\\n')\n",
        "    for line in lines:\n",
        "        if ':' in line:\n",
        "            key, value = line.split(':', 1)\n",
        "            params[key.strip()] = value.strip()\n",
        "    return params\n",
        "\n",
        "# Define custom compute_metrics function\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    # Replace -100 with pad_token_id\n",
        "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # Decode predictions and references without special tokens\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    for p, l in zip(pred_str, label_str):\n",
        "        print(\"Prediction:\", p)\n",
        "        print(\"Reference:\", l)\n",
        "        print(\"---\")\n",
        "\n",
        "    total = len(pred_str)\n",
        "    exact_matches = sum([1 for p, l in zip(pred_str, label_str) if p.strip() == l.strip()])\n",
        "    exact_match_accuracy = exact_matches / total\n",
        "\n",
        "    # Parameter-level accuracy\n",
        "    parameter_accuracy = {}\n",
        "    for param in [\"Waveform\", \"Voices\", \"Oscillator Detune\", \"Filter Type\", \"Filter Cutoff\", \"ADSR Envelope\", \"LFO Modulation\"]:\n",
        "        correct = 0\n",
        "        for p, l in zip(pred_str, label_str):\n",
        "            pred_params = parse_parameters(p)\n",
        "            label_params = parse_parameters(l)\n",
        "            if pred_params.get(param) == label_params.get(param):\n",
        "                correct += 1\n",
        "        parameter_accuracy[param] = correct / total\n",
        "\n",
        "    # Combine metrics\n",
        "    metrics = {\"exact_match_accuracy\": exact_match_accuracy}\n",
        "    metrics.update(parameter_accuracy)\n",
        "    return metrics\n",
        "\n",
        "# Define the data collator\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "    decoder_start_token_id: int\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # Separate input_features and labels\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        labels = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        # Pad input_features using the feature extractor\n",
        "        batch = self.processor.feature_extractor.pad(\n",
        "            input_features,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Pad labels using the tokenizer\n",
        "        labels_batch = self.processor.tokenizer.pad(\n",
        "            labels,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Replace padding token id's of the labels by -100\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
        "            labels_batch.attention_mask.ne(1), -100\n",
        "        )\n",
        "\n",
        "        # Remove the decoder_start_token_id\n",
        "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "# Initialize the data collator\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
        "    processor=processor,\n",
        "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
        ")\n",
        "\n",
        "# Define Trainer for evaluation\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/whisper_synth_files/eval_logs\",\n",
        "    per_device_eval_batch_size=4,\n",
        "    predict_with_generate=True,\n",
        "    eval_strategy=\"no\",  # Set to \"no\" as we're only evaluating here\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Perform evaluation\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Print evaluation results\n",
        "print(eval_results)\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "9n-hxSDcrKtP"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate the environment and run the script\n",
        "!source synth_env/bin/activate && python eval.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBtlBgjjrdaS",
        "outputId": "34cfa5c2-d3ae-430a-d275-5bb2a02e4542"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map: 100% 40/40 [00:01<00:00, 37.21 examples/s]\n",
            "/content/eval.py:183: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n",
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "  0% 0/10 [00:00<?, ?it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            " 20% 2/10 [00:05<00:23,  2.94s/it]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            " 30% 3/10 [00:11<00:29,  4.17s/it]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            " 40% 4/10 [00:17<00:28,  4.79s/it]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            " 50% 5/10 [00:23<00:25,  5.18s/it]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            " 60% 6/10 [00:29<00:21,  5.39s/it]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            " 70% 7/10 [00:35<00:16,  5.53s/it]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            " 80% 8/10 [00:40<00:11,  5.61s/it]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            " 90% 9/10 [00:46<00:05,  5.67s/it]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "100% 10/10 [00:52<00:00,  5.55s/it]Prediction:  Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs\n",
            "Reference: Waveform: Sine\n",
            "Voices: 1\n",
            "Oscillator Detune: None\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 500ms, Decay: 1.00s, Sustain: -6 dB, Release: 300ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction: \n",
            "Reference: Waveform: Square\n",
            "Voices: 3\n",
            "Oscillator Detune: 0.25\n",
            "Filter Type: Highpass\n",
            "Filter Cutoff: 7667Hz\n",
            "ADSR Envelope: Attack: 481ms, Decay: 163ms, Sustain: -9.8 dB, Release: 920ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Saw\n",
            "Voices: 1\n",
            "Oscillator Detune: None\n",
            "Filter Type: Bandpass\n",
            "Filter Cutoff: 500Hz\n",
            "ADSR Envelope: Attack: 200ms, Decay: 800ms, Sustain: -9 dB, Release: 400ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  Sound Effects\n",
            "Reference: Waveform: Sine\n",
            "Voices: 1\n",
            "Oscillator Detune: None\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 700ms, Decay: 1.80s, Sustain: -6 dB, Release: 1.00s\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Triangle\n",
            "Voices: 2\n",
            "Oscillator Detune: 0.10\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 200ms, Decay: 1.20s, Sustain: -9 dB, Release: 500ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Triangle\n",
            "Voices: 2\n",
            "Oscillator Detune: .05\n",
            "Filter Type: Lowpass\n",
            "Filter Cutoff: 800 Hz\n",
            "ADSR Envelope: Attack: 10ms, Decay: 800ms, Sustain: -3 dB, Release: 50ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Triangle\n",
            "Voices: 1\n",
            "Oscillator Detune: None\n",
            "Filter Type: Lowpass\n",
            "Filter Cutoff: 5000Hz\n",
            "ADSR Envelope: Attack: 500ms, Decay: 1.00s, Sustain: -6 dB, Release: 300ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  Sound Effects\n",
            "Reference: Waveform: Sine\n",
            "Voices: 8\n",
            "Oscillator Detune: 0.11\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 236ms, Decay: 110ms, Sustain: -11.6 dB, Release: 829ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Pulse\n",
            "Voices: 5\n",
            "Oscillator Detune: 0.10\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 200ms, Decay: 1.20s, Sustain: -9 dB, Release: 500ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction: \n",
            "Reference: Waveform: Sine\n",
            "Voices: 2\n",
            "Oscillator Detune: 0.15\n",
            "Filter Type: Highpass\n",
            "Filter Cutoff: 19383Hz\n",
            "ADSR Envelope: Attack: 885ms, Decay: 1.88s, Sustain: -51.3 dB, Release: 1.93s\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Pulse\n",
            "Voices: 3\n",
            "Oscillator Detune: 0.10\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 200ms, Decay: 1.20s, Sustain: -9 dB, Release: 500ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  NOOOOO!!!\n",
            "Reference: Waveform: Pulse\n",
            "Voices: 7\n",
            "Oscillator Detune: 0.07\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 633ms, Decay: 228ms, Sustain: -2.5 dB, Release: 800ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction: \n",
            "Reference: Waveform: Sine\n",
            "Voices: 6\n",
            "Oscillator Detune: 0.05\n",
            "Filter Type: Highpass\n",
            "Filter Cutoff: 6082Hz\n",
            "ADSR Envelope: Attack: 38ms, Decay: 946ms, Sustain: -7.4 dB, Release: 174ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction: \n",
            "Reference: Waveform: Saw\n",
            "Voices: 1\n",
            "Oscillator Detune: None\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 200ms, Decay: 1.20s, Sustain: -9 dB, Release: 500ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Square\n",
            "Voices: 1\n",
            "Oscillator Detune: None\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 0.5ms, Decay: 1.00s, Sustain: 0 dB, Release: 15ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Square\n",
            "Voices: 1\n",
            "Oscillator Detune: None\n",
            "Filter Type: Bandpass\n",
            "Filter Cutoff: 368Hz\n",
            "ADSR Envelope: Attack: 244ms, Decay: 583ms, Sustain: -0.6 dB, Release: 894ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Saw\n",
            "Voices: 6\n",
            "Oscillator Detune: .30\n",
            "Filter Type: Lowpass\n",
            "Filter Cutoff: 700 Hz\n",
            "ADSR Envelope: Attack: 0.3ms, Decay: 1.30s, Sustain: -6 dB, Release: 75ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Saw\n",
            "Voices: 3\n",
            "Oscillator Detune: 0.40\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 346ms, Decay: 808ms, Sustain: -2.8 dB, Release: 595ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Pulse\n",
            "Voices: 1\n",
            "Oscillator Detune: None\n",
            "Filter Type: Lowpass\n",
            "Filter Cutoff: 1000Hz\n",
            "ADSR Envelope: Attack: 300ms, Decay: 800ms, Sustain: -12 dB, Release: 400ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction: \n",
            "Reference: Waveform: Square\n",
            "Voices: 3\n",
            "Oscillator Detune: 0.59\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 1.13s, Decay: 1.04s, Sustain: -37.7 dB, Release: 768ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  Sound Effects\n",
            "Reference: Waveform: Sine\n",
            "Voices: 6\n",
            "Oscillator Detune: 0.25\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 600ms, Decay: 1.80s, Sustain: -9 dB, Release: 800ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Pulse\n",
            "Voices: 1\n",
            "Oscillator Detune: None\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 200ms, Decay: 500ms, Sustain: -3 dB, Release: 400ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Triangle\n",
            "Voices: 3\n",
            "Oscillator Detune: .26\n",
            "Filter Type: Bandpass\n",
            "Filter Cutoff: 1 kHz\n",
            "ADSR Envelope: Attack: 5ms, Decay: 500ms, Sustain: -9 dB, Release: 50ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Square\n",
            "Voices: 7\n",
            "Oscillator Detune: 0.33\n",
            "Filter Type: Lowpass\n",
            "Filter Cutoff: 9016Hz\n",
            "ADSR Envelope: Attack: 635ms, Decay: 821ms, Sustain: -6.2 dB, Release: 82ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Square\n",
            "Voices: 1\n",
            "Oscillator Detune: None\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 0.00s, Decay: 2.00s, Sustain: -3 dB, Release: 800ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Saw\n",
            "Voices: 3\n",
            "Oscillator Detune: 0.11\n",
            "Filter Type: Bandpass\n",
            "Filter Cutoff: 2091Hz\n",
            "ADSR Envelope: Attack: 880ms, Decay: 1.01s, Sustain: -15.4 dB, Release: 315ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction: \n",
            "Reference: Waveform: Triangle\n",
            "Voices: 2\n",
            "Oscillator Detune: 0.15\n",
            "Filter Type: Bandpass\n",
            "Filter Cutoff: 5470Hz\n",
            "ADSR Envelope: Attack: 238ms, Decay: 836ms, Sustain: -3.5 dB, Release: 934ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Triangle\n",
            "Voices: 8\n",
            "Oscillator Detune: 0.16\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 723ms, Decay: 846ms, Sustain: -10.5 dB, Release: 561ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction: \n",
            "Reference: Waveform: Triangle\n",
            "Voices: 7\n",
            "Oscillator Detune: 0.30\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 700ms, Decay: 900ms, Sustain: -12 dB, Release: 900ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction: \n",
            "Reference: Waveform: Saw\n",
            "Voices: 1\n",
            "Oscillator Detune: None\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 50ms, Decay: 700ms, Sustain: -3 dB, Release: 300ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Square\n",
            "Voices: 4\n",
            "Oscillator Detune: 0.37\n",
            "Filter Type: Lowpass\n",
            "Filter Cutoff: 14409Hz\n",
            "ADSR Envelope: Attack: 650ms, Decay: 795ms, Sustain: -7.8 dB, Release: 51ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction: \n",
            "Reference: Waveform: Saw\n",
            "Voices: 1\n",
            "Oscillator Detune: None\n",
            "Filter Type: Highpass\n",
            "Filter Cutoff: 7261Hz\n",
            "ADSR Envelope: Attack: 735ms, Decay: 782ms, Sustain: -7.9 dB, Release: 663ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Square\n",
            "Voices: 4\n",
            "Oscillator Detune: 0.10\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 200ms, Decay: 1.20s, Sustain: -9 dB, Release: 500ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Square\n",
            "Voices: 2\n",
            "Oscillator Detune: 0.88\n",
            "Filter Type: Highpass\n",
            "Filter Cutoff: 133Hz\n",
            "ADSR Envelope: Attack: 219ms, Decay: 1.27s, Sustain: -31.0 dB, Release: 1.53s\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Triangle\n",
            "Voices: 2\n",
            "Oscillator Detune: 0.05\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 200ms, Decay: 500ms, Sustain: -6 dB, Release: 300ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  Sound Effects\n",
            "Reference: Waveform: Sine\n",
            "Voices: 7\n",
            "Oscillator Detune: 0.73\n",
            "Filter Type: Bandpass\n",
            "Filter Cutoff: 7102Hz\n",
            "ADSR Envelope: Attack: 457ms, Decay: 149ms, Sustain: -38.1 dB, Release: 1.48s\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction: \n",
            "Reference: Waveform: Square\n",
            "Voices: 7\n",
            "Oscillator Detune: 0.49\n",
            "Filter Type: Bandpass\n",
            "Filter Cutoff: 3536Hz\n",
            "ADSR Envelope: Attack: 960ms, Decay: 461ms, Sustain: -10.9 dB, Release: 228ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction: \n",
            "Reference: Waveform: Triangle\n",
            "Voices: 3\n",
            "Oscillator Detune: 0.30\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 85ms, Decay: 243ms, Sustain: -6.3 dB, Release: 156ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Square\n",
            "Voices: 1\n",
            "Oscillator Detune: None\n",
            "Filter Type: Lowpass\n",
            "Filter Cutoff: 10000Hz\n",
            "ADSR Envelope: Attack: 500ms, Decay: 1.00s, Sustain: -6 dB, Release: 300ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "Prediction:  \n",
            "Reference: Waveform: Square\n",
            "Voices: 3\n",
            "Oscillator Detune: 0.11\n",
            "Filter Type: N/A\n",
            "Filter Cutoff: N/A\n",
            "ADSR Envelope: Attack: 735ms, Decay: 959ms, Sustain: -10.9 dB, Release: 671ms\n",
            "LFO Modulation: None\n",
            "---\n",
            "100% 10/10 [01:03<00:00,  6.32s/it]\n",
            "{'eval_loss': 7.573992729187012, 'eval_model_preparation_time': 0.006, 'eval_exact_match_accuracy': 0.0, 'eval_Waveform': 0.0, 'eval_Voices': 0.0, 'eval_Oscillator Detune': 0.0, 'eval_Filter Type': 0.0, 'eval_Filter Cutoff': 0.0, 'eval_ADSR Envelope': 0.0, 'eval_LFO Modulation': 0.0, 'eval_runtime': 70.4598, 'eval_samples_per_second': 0.568, 'eval_steps_per_second': 0.142}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SCRIPT FOR INFERENCE (inference.py)\n",
        "\n",
        "# Create a script to perform evaluation\n",
        "with open(\"inference.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import (\n",
        "    WhisperForConditionalGeneration,\n",
        "    WhisperProcessor,\n",
        "    WhisperTokenizer\n",
        ")\n",
        "\n",
        "# Path to your combined model directory\n",
        "model_dir = \"/content/drive/MyDrive/whisper_synth_files/whisper_synth\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "# Load the processor with the tokenizer\n",
        "processor = WhisperProcessor.from_pretrained(\n",
        "    model_dir,\n",
        "    language=\"en\",\n",
        "    task=\"transcribe\",\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Load the combined model\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_dir)\n",
        "\n",
        "# Force the model to generate in English\n",
        "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
        "    language=\"en\", task=\"transcribe\"\n",
        ")\n",
        "\n",
        "# Set model to evaluation mode and move to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load and process the audio file\n",
        "audio_path = \"/content/drive/MyDrive/whisper_synth_files/data/inference/inference1.wav\"  # Replace with the path to your .wav file\n",
        "audio_array, sampling_rate = torchaudio.load(audio_path)\n",
        "\n",
        "# Resample to 16 kHz if necessary\n",
        "if sampling_rate != 16000:\n",
        "    resampler = torchaudio.transforms.Resample(\n",
        "        orig_freq=sampling_rate, new_freq=16000\n",
        "    )\n",
        "    audio_array = resampler(audio_array)\n",
        "\n",
        "# If the audio has multiple channels, convert it to mono\n",
        "if audio_array.shape[0] > 1:\n",
        "    audio_array = torch.mean(audio_array, dim=0, keepdim=True)\n",
        "\n",
        "audio_array = audio_array.squeeze().numpy()\n",
        "\n",
        "# Process audio with the feature extractor to get input features\n",
        "input_features = processor.feature_extractor(\n",
        "    audio_array, sampling_rate=16000, return_tensors=\"pt\"\n",
        ").input_features.to(device)\n",
        "\n",
        "# Generate synth_patch\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(input_features)\n",
        "\n",
        "# Decode the generated IDs to get the synth_patch\n",
        "synth_patch = processor.tokenizer.batch_decode(\n",
        "    generated_ids, skip_special_tokens=True\n",
        ")[0]\n",
        "print(\"Synth Patch:\", synth_patch)\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "pu6LuvdoHZdz"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate the environment and run the script\n",
        "!source synth_env/bin/activate && python inference.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8apNU6NLIgQQ",
        "outputId": "93e13f68-79e7-4219-aee5-acc915ce635e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Synth Patch:  Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs Ndhs\n"
          ]
        }
      ]
    }
  ]
}